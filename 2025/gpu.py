import torch
import time
import sys


def print_separator(title):
    print("\n" + "=" * 60)
    print(f" {title}")
    print("=" * 60)


def test_basic_gpu_info():
    """æµ‹è¯•åŸºæœ¬ GPU ä¿¡æ¯"""
    print_separator("1. åŸºæœ¬ GPU ä¿¡æ¯")

    # CUDA å¯ç”¨æ€§
    cuda_available = torch.cuda.is_available()
    print(f"âœ… CUDA å¯ç”¨: {cuda_available}")

    if not cuda_available:
        print("âŒ CUDA ä¸å¯ç”¨ï¼Œæµ‹è¯•ç»ˆæ­¢")
        return False

    # CUDA ç‰ˆæœ¬ä¿¡æ¯
    print(f"âœ… PyTorch CUDA ç‰ˆæœ¬: {torch.version.cuda}")
    print(f"âœ… cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}")

    # GPU æ•°é‡å’Œä¿¡æ¯
    gpu_count = torch.cuda.device_count()
    print(f"âœ… å¯ç”¨ GPU æ•°é‡: {gpu_count}")

    for i in range(gpu_count):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_props = torch.cuda.get_device_properties(i)
        print(f"   GPU {i}: {gpu_name}")
        print(f"     - æ˜¾å­˜æ€»é‡: {gpu_props.total_memory / 1024 ** 3:.2f} GB")
        print(f"     - è®¡ç®—èƒ½åŠ›: {gpu_props.major}.{gpu_props.minor}")
        print(f"     - å¤šå¤„ç†å™¨æ•°é‡: {gpu_props.multi_processor_count}")

    return True


def test_gpu_tensor_operations():
    """æµ‹è¯• GPU å¼ é‡æ“ä½œ"""
    print_separator("2. GPU å¼ é‡æ“ä½œæµ‹è¯•")

    # åˆ›å»ºå¼ é‡å¹¶ç§»åŠ¨åˆ° GPU
    device = torch.device('cuda')

    # åŸºæœ¬å¼ é‡æ“ä½œ
    x = torch.randn(1000, 1000).to(device)
    y = torch.randn(1000, 1000).to(device)

    print(f"âœ… å¼ é‡åˆ›å»ºæˆåŠŸ")
    print(f"   x å½¢çŠ¶: {x.shape}, è®¾å¤‡: {x.device}")
    print(f"   y å½¢çŠ¶: {y.shape}, è®¾å¤‡: {y.device}")

    # å„ç§è¿ç®—æµ‹è¯•
    z_add = x + y
    z_matmul = torch.matmul(x, y)
    z_sum = torch.sum(x)

    print(f"âœ… åŸºæœ¬è¿ç®—æµ‹è¯•é€šè¿‡")
    print(f"   åŠ æ³•ç»“æœå½¢çŠ¶: {z_add.shape}")
    print(f"   çŸ©é˜µä¹˜æ³•å½¢çŠ¶: {z_matmul.shape}")
    print(f"   æ±‚å’Œç»“æœ: {z_sum.item():.4f}")

    return True


def test_gpu_performance():
    """æµ‹è¯• GPU æ€§èƒ½"""
    print_separator("3. GPU æ€§èƒ½æµ‹è¯•")

    device = torch.device('cuda')

    # åˆ›å»ºæ›´å¤§çš„å¼ é‡è¿›è¡Œæ€§èƒ½æµ‹è¯•
    size = 5000
    a = torch.randn(size, size).to(device)
    b = torch.randn(size, size).to(device)

    # é¢„çƒ­ GPU
    _ = torch.matmul(a, b)
    torch.cuda.synchronize()  # ç­‰å¾… GPU å®Œæˆ

    # è®¡æ—¶çŸ©é˜µä¹˜æ³•
    start_time = time.time()
    c = torch.matmul(a, b)
    torch.cuda.synchronize()  # ç¡®ä¿è®¡ç®—å®Œæˆ
    end_time = time.time()

    gpu_time = end_time - start_time

    print(f"âœ… GPU çŸ©é˜µä¹˜æ³•æµ‹è¯•å®Œæˆ")
    print(f"   çŸ©é˜µå¤§å°: {size} x {size}")
    print(f"   GPU è®¡ç®—æ—¶é—´: {gpu_time:.4f} ç§’")

    # å¯¹æ¯” CPU æ€§èƒ½ï¼ˆå¯é€‰ï¼‰
    if torch.cuda.device_count() == 0:  # å¦‚æœæ²¡æœ‰ GPUï¼Œè·³è¿‡å¯¹æ¯”
        return True

    print("\n--- ä¸ CPU å¯¹æ¯” ---")
    a_cpu = a.cpu()
    b_cpu = b.cpu()

    start_time_cpu = time.time()
    c_cpu = torch.matmul(a_cpu, b_cpu)
    end_time_cpu = time.time()

    cpu_time = end_time_cpu - start_time_cpu
    print(f"   CPU è®¡ç®—æ—¶é—´: {cpu_time:.4f} ç§’")
    print(f"   GPU åŠ é€Ÿæ¯”: {cpu_time / gpu_time:.2f}x")

    return True


def test_gpu_memory():
    """æµ‹è¯• GPU å†…å­˜ç®¡ç†"""
    print_separator("4. GPU å†…å­˜ç®¡ç†æµ‹è¯•")

    if not torch.cuda.is_available():
        return False

    # åˆå§‹å†…å­˜çŠ¶æ€
    initial_allocated = torch.cuda.memory_allocated()
    initial_cached = torch.cuda.memory_reserved()

    print(f"åˆå§‹æ˜¾å­˜ä½¿ç”¨: {initial_allocated / 1024 ** 2:.2f} MB")
    print(f"åˆå§‹ç¼“å­˜æ˜¾å­˜: {initial_cached / 1024 ** 2:.2f} MB")

    # åˆ†é…å¤§å—å†…å­˜
    device = torch.device('cuda')
    large_tensor = torch.randn(5000, 5000).to(device)

    allocated_after = torch.cuda.memory_allocated()
    cached_after = torch.cuda.memory_reserved()

    print(f"åˆ†é…åæ˜¾å­˜ä½¿ç”¨: {allocated_after / 1024 ** 2:.2f} MB")
    print(f"åˆ†é…åç¼“å­˜æ˜¾å­˜: {cached_after / 1024 ** 2:.2f} MB")

    # æ¸…ç†
    del large_tensor
    torch.cuda.empty_cache()

    final_allocated = torch.cuda.memory_allocated()
    final_cached = torch.cuda.memory_reserved()

    print(f"æ¸…ç†åæ˜¾å­˜ä½¿ç”¨: {final_allocated / 1024 ** 2:.2f} MB")
    print(f"æ¸…ç†åç¼“å­˜æ˜¾å­˜: {final_cached / 1024 ** 2:.2f} MB")

    print("âœ… GPU å†…å­˜ç®¡ç†æµ‹è¯•é€šè¿‡")
    return True


def test_neural_network_gpu():
    """æµ‹è¯•ç¥ç»ç½‘ç»œåœ¨ GPU ä¸Šçš„è¿è¡Œ"""
    print_separator("5. ç¥ç»ç½‘ç»œ GPU æµ‹è¯•")

    if not torch.cuda.is_available():
        return False

    device = torch.device('cuda')

    # åˆ›å»ºä¸€ä¸ªç®€å•çš„ CNN æ¨¡å‹
    class SimpleCNN(torch.nn.Module):
        def __init__(self):
            super(SimpleCNN, self).__init__()
            self.conv1 = torch.nn.Conv2d(3, 32, kernel_size=3, padding=1)
            self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=3, padding=1)
            self.fc1 = torch.nn.Linear(64 * 8 * 8, 256)
            self.fc2 = torch.nn.Linear(256, 10)
            self.pool = torch.nn.MaxPool2d(2, 2)
            self.relu = torch.nn.ReLU()

        def forward(self, x):
            x = self.pool(self.relu(self.conv1(x)))
            x = self.pool(self.relu(self.conv2(x)))
            x = x.view(-1, 64 * 8 * 8)
            x = self.relu(self.fc1(x))
            x = self.fc2(x)
            return x

    # åˆ›å»ºæ¨¡å‹å¹¶ç§»åŠ¨åˆ° GPU
    model = SimpleCNN().to(device)
    print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œè®¾å¤‡: {next(model.parameters()).device}")

    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    batch_size = 32
    dummy_input = torch.randn(batch_size, 3, 32, 32).to(device)
    dummy_target = torch.randint(0, 10, (batch_size,)).to(device)

    # å‰å‘ä¼ æ’­
    output = model(dummy_input)
    print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
    print(f"   è¾“å…¥å½¢çŠ¶: {dummy_input.shape}")
    print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")

    # è®¡ç®—æŸå¤±
    criterion = torch.nn.CrossEntropyLoss()
    loss = criterion(output, dummy_target)
    print(f"âœ… æŸå¤±è®¡ç®—æˆåŠŸ: {loss.item():.4f}")

    # åå‘ä¼ æ’­
    model.zero_grad()
    loss.backward()
    print("âœ… åå‘ä¼ æ’­æˆåŠŸ")

    # æ£€æŸ¥æ¢¯åº¦
    has_gradients = any(p.grad is not None for p in model.parameters())
    print(f"âœ… æ¢¯åº¦è®¡ç®—: {'æˆåŠŸ' if has_gradients else 'å¤±è´¥'}")

    return True


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print_separator("PyTorch GPU å…¨é¢æµ‹è¯•")
    print(f"Python ç‰ˆæœ¬: {sys.version}")
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")

    all_tests_passed = True

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        test_basic_gpu_info,
        test_gpu_tensor_operations,
        test_gpu_performance,
        test_gpu_memory,
        test_neural_network_gpu
    ]

    for test in tests:
        try:
            result = test()
            if not result:
                all_tests_passed = False
                print(f"âŒ {test.__name__} å¤±è´¥")
        except Exception as e:
            print(f"âŒ {test.__name__} å‡ºé”™: {e}")
            all_tests_passed = False

    print_separator("æµ‹è¯•æ€»ç»“")
    if all_tests_passed:
        print("ğŸ‰ æ‰€æœ‰ GPU æµ‹è¯•é€šè¿‡ï¼PyTorch GPU ç‰ˆæœ¬å®‰è£…æˆåŠŸï¼")
        print("æ‚¨ç°åœ¨å¯ä»¥æ­£å¸¸ä½¿ç”¨ GPU è¿›è¡Œæ·±åº¦å­¦ä¹ è®­ç»ƒäº†ã€‚")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ PyTorch GPU å®‰è£…ã€‚")
        print("å¯èƒ½çš„é—®é¢˜ï¼š")
        print("  - CUDA é©±åŠ¨ç‰ˆæœ¬ä¸åŒ¹é…")
        print("  - PyTorch ç‰ˆæœ¬ä¸ CUDA ç‰ˆæœ¬ä¸å…¼å®¹")
        print("  - GPU å†…å­˜ä¸è¶³")
        print("  - ç³»ç»Ÿæƒé™é—®é¢˜")

    return all_tests_passed


if __name__ == "__main__":
    main()
